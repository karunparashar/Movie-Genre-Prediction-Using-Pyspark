{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# sentenceDataFrame = spark.createDataFrame -- to create dataframe\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "import numpy as np\n",
    "#Vector assembler , string indexer and index to string from spark ML\n",
    "# spark.driver.maxResultSize\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"16g\")\\\n",
    "        .config(\"spark.driver.memory\", \"16g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"8g\")\\\n",
    "        .getOrCreate()\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv file (only for version 1.4 and above--current version is 2.4.5 BTW)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/karunparashar/Downloads/Assignment 3 DIC/train2.csv')\n",
    "dataframe = sqlContext.createDataFrame(df)\n",
    "df2 = pd.read_csv('/Users/karunparashar/Downloads/Assignment 3 DIC/dic487-587/test.csv')\n",
    "dataframe2 = sqlContext.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",'br','find','one', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
    "regexTokenizer2 = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover2 = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total time taken-35 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "mapping_dict = {'Drama':0,\n",
    "               'Comedy':1,\n",
    "               'Romance Film':2,\n",
    "               'Thriller':3,\n",
    "               'Action':4,\n",
    "               'World cinema':5,\n",
    "               'Crime Fiction':6,\n",
    "               'Horror':7,\n",
    "               'Black-and-white':8,\n",
    "               'Indie':9,\n",
    "               'Action/Adventure':10,\n",
    "               'Adventure':11,\n",
    "               'Family Film':12,\n",
    "               'Short Film':13,\n",
    "               'Romantic drama':14,\n",
    "               'Animation':15,\n",
    "               'Musical':16,\n",
    "               'Science Fiction':17,\n",
    "               'Mystery':18,\n",
    "               'Romantic comedy':19}\n",
    "nd_TFIDF ={}\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=500)\n",
    "for i in mapping_dict:\n",
    "    udf = UserDefinedFunction(lambda x: 1 if i in x else 0 , IntegerType())\n",
    "    new_df = dataframe.withColumn('label', udf(dataframe.genre))\n",
    "    pipeline = Pipeline(stages=[regexTokenizer2, stopwordsRemover2, hashingTF, idf])\n",
    "    pipelineFit = pipeline.fit(new_df)\n",
    "    dataset = pipelineFit.transform(new_df)\n",
    "    dataframe_test_1 =pipelineFit.transform(dataframe2) \n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0)\n",
    "    lrModel = lr.fit(dataset)\n",
    "    predictions = lrModel.transform(dataframe_test_1)\n",
    "    nd_TFIDF[i] = np.array(predictions.select('prediction').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how the prediction stored for each genre looks like\n",
    "for i in nd_TFIDF:\n",
    "    nd_TFIDF[i] = list(nd_TFIDF[i])\n",
    "nd_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dictionary to dataframe for output compatible format store in CSV file,pandas has been used only for this purpose\n",
    "a1 =pd.DataFrame(nd_TFIDF).astype(int)\n",
    "predds = pd.Series(list(a1.iloc[i,]) for i in range(len(a1)))#to do in spark\n",
    "pd.DataFrame(predds).to_csv('intermediate_TFIDF_logreg_16.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
