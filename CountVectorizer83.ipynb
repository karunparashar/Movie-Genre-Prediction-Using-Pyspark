{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks:\n",
    "## 1. Pipeline containing regex tokenization, removal of stopwords and feature vector creation using\n",
    "###     a. Count vectorizer\n",
    "###     b. Hashing TF and IDF\n",
    "###     c. Word2 Vector\n",
    "## 2. Conversion of mulitlabel classification into 20 binary classification problems via\n",
    "###     a.Parsing through the list of available labels\n",
    "###     b.Checking if the label is a part of a row- if yes then replace that label by 1, otherwise 0\n",
    "## 3. Fitting logistic regression and converting to output format compatible dataframe\n",
    "###     a.Storing the prediction column for each model in a dictionary\n",
    "###     b.Converting the dictionary into dataframe and converting it to output compatible csv file\n",
    "###     c.Storing 3 different csvs for three feature vectorization methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell only once per notebook session\n",
    "\n",
    "\n",
    "Specifically, line 13 to be run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# sentenceDataFrame = spark.createDataFrame -- to create dataframe\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "import numpy as np\n",
    "#Vector assembler , string indexer and index to string from spark ML\n",
    "# spark.driver.maxResultSize\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"16g\")\\\n",
    "        .config(\"spark.driver.memory\", \"16g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"8g\")\\\n",
    "        .getOrCreate()\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pyspark --driver-class-path /usr/share/java/mysql-connector-java.jar\n",
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv file (only for version 1.4 and above--current version is 2.4.5 BTW)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/karunparashar/Downloads/Assignment 3 DIC/train2.csv')\n",
    "dataframe = sqlContext.createDataFrame(df)\n",
    "df2 = pd.read_csv('/Users/karunparashar/Downloads/Assignment 3 DIC/dic487-587/test.csv')\n",
    "dataframe2 = sqlContext.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to remove stopwords from the plot column using in build stop words from Spark\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\",vocabSize=10000, outputCol=\"features\",minDF=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using count vectorizer-20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "mapping_dict = {'Drama':0,\n",
    "               'Comedy':1,\n",
    "               'Romance Film':2,\n",
    "               'Thriller':3,\n",
    "               'Action':4,\n",
    "               'World cinema':5,\n",
    "               'Crime Fiction':6,\n",
    "               'Horror':7,\n",
    "               'Black-and-white':8,\n",
    "               'Indie':9,\n",
    "               'Action/Adventure':10,\n",
    "               'Adventure':11,\n",
    "               'Family Film':12,\n",
    "               'Short Film':13,\n",
    "               'Romantic drama':14,\n",
    "               'Animation':15,\n",
    "               'Musical':16,\n",
    "               'Science Fiction':17,\n",
    "               'Mystery':18,\n",
    "               'Romantic comedy':19}\n",
    "nd1={}\n",
    "for i in mapping_dict:\n",
    "    udf = UserDefinedFunction(lambda x: 1 if i in x else 0 , IntegerType())\n",
    "    new_df = dataframe.withColumn('label', udf(dataframe.genre))\n",
    "    pipeline = Pipeline(stages=[regexTokenizer,stopwordsRemover, countVectors])\n",
    "    pipelineFit = pipeline.fit(new_df)\n",
    "    dataset = pipelineFit.transform(new_df)\n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0)\n",
    "    lrModel = lr.fit(dataset)\n",
    "    dataframetest_0 =pipelineFit.transform(dataframe2) \n",
    "    pred1 = lrModel.transform(dataframetest_0)\n",
    "    nd1[i] = np.array(pred1.select('prediction').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how the prediction stored for each genre looks like\n",
    "for i in nd1:\n",
    "    nd1[i] = list(nd1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#converting dictionary to dataframe for output compatible format store in CSV file,pandas has been used only for this purpose\n",
    "a =pd.DataFrame(nd1).astype(int)\n",
    "predds = pd.Series(list(a.iloc[i,]) for i in range(len(a)))\n",
    "pd.DataFrame(predds).to_csv('intermediate_count_vectorizerlogreg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
